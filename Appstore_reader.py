# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpsqNJpU-VVa5hw_mBAJWpcX-EHsAs7B
"""

# Google Colab — single-cell, standard-library-only script
# --------------------------------------------------------
# What this uses:
# Apple’s public Customer Reviews Atom/RSS endpoint (no keys, no login).
# Endpoint pattern (US storefront):
#   https://itunes.apple.com/us/rss/customerreviews/page={PAGE}/id={APP_ID}/sortby=mostrecent/xml
#
# Pagination:
# - `page` is 1-based.
# - We keep fetching pages until we collect MAX_REVIEWS or the feed returns no review entries.
#
# Notes:
# - Apple’s reviews feed is Atom-like XML with namespaces: Atom + iTunes.
# - The first <entry> is often the app metadata (not a review). We skip entries without rating/body.
# - “Device type” (iPhone/iPad/Mac/Apple Silicon) is generally NOT exposed in this public feed.
#   If Apple includes any device/platform hints in the future (rare), this script will capture them
#   as additional columns when present.

import re
import csv
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime, timezone
import xml.etree.ElementTree as ET

# ---------------------------
# User inputs / configuration
# ---------------------------
APP_URL = ""  # e.g. "https://apps.apple.com/us/app/some-app/id123456789"
MAX_REVIEWS = 100
STOREFRONT = "us"  # fixed per requirements

# If APP_URL is empty, read from input()
if not APP_URL.strip():
    try:
        APP_URL = input("Paste Apple App Store URL (apps.apple.com/...): ").strip()
    except Exception:
        print("Failed to read input()")
        sys.exit(1)

# ---------------------------
# Namespaces for Atom/iTunes
# ---------------------------
NS = {
    "atom": "http://www.w3.org/2005/Atom",
    "im": "http://itunes.apple.com/rss",
    "itunes": "http://www.itunes.com/dtds/podcast-1.0.dtd",  # not always used, but safe to include
    # Some feeds include these:
    "thr": "http://purl.org/syndication/thread/1.0",
    "gd": "http://schemas.google.com/g/2005",
}

# ---------------------------
# Helpers
# ---------------------------
def extract_app_id(url: str) -> str:
    """
    Extract numeric app_id from Apple App Store URL.
    Supports:
      - https://apps.apple.com/us/app/some-app/id123456789
      - https://apps.apple.com/app/id123456789
    Robustly finds substring id\\d+ anywhere in the URL.
    """
    if not isinstance(url, str) or not url.strip():
        raise ValueError("APP_URL is empty")

    m = re.search(r"id(\d+)", url)
    if not m:
        raise ValueError(f"Could not find app_id in URL: {url}")
    return m.group(1)


def _http_get(url: str, timeout: int = 20) -> str:
    """
    Simple HTTP GET with urllib.request (standard library).
    Returns decoded text (utf-8).
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; ColabPythonScript/1.0; +https://colab.research.google.com/)",
        "Accept": "application/xml,text/xml,application/atom+xml,*/*",
    }
    req = urllib.request.Request(url, headers=headers, method="GET")
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        raw = resp.read()
    # The feed is typically UTF-8; if not, decode with replacement.
    return raw.decode("utf-8", errors="replace")


def fetch_feed(app_id: str, page: int) -> str:
    """
    Fetch a single page of the Apple customer reviews feed (US storefront).
    Endpoint example:
      https://itunes.apple.com/us/rss/customerreviews/page=1/id=123456789/sortby=mostrecent/xml
    """
    feed_url = (
        f"https://itunes.apple.com/{STOREFRONT}/rss/customerreviews/"
        f"page={page}/id={app_id}/sortby=mostrecent/xml"
    )
    return _http_get(feed_url)


def _strip_ns(tag: str) -> str:
    """Convert '{namespace}local' -> 'local' for display/debug."""
    if tag.startswith("{") and "}" in tag:
        return tag.split("}", 1)[1]
    return tag


def _find_text(elem, path, default=""):
    """
    Find subelement text with namespaces; return default if not found.
    `path` should include prefixes like 'atom:title' etc.
    """
    found = elem.find(path, NS)
    if found is None or found.text is None:
        return default
    return found.text.strip()


def _find_attr(elem, path, attr_name, default=""):
    found = elem.find(path, NS)
    if found is None:
        return default
    return (found.attrib.get(attr_name) or "").strip() or default


def _collect_extra_fields(entry):
    """
    Collect additional fields that might appear in entry beyond the minimal set.
    This helps satisfy: "выгрузи “все поля”, которые удастся извлечь из фида".
    We store namespaced tags as 'prefix:local' where possible.
    """
    extras = {}

    # Iterate direct children only (avoid deep recursion to keep it readable)
    for child in list(entry):
        tag = child.tag
        text = (child.text or "").strip()

        # Skip huge body duplication - we already capture content
        local = _strip_ns(tag)

        # Try to identify a prefix from known namespaces
        prefix = None
        if tag.startswith("{"):
            ns_uri = tag[1:].split("}", 1)[0]
            for k, v in NS.items():
                if v == ns_uri:
                    prefix = k
                    break

        key_base = f"{prefix}:{local}" if prefix else local

        # Some children are structures with attributes or nested text
        if len(list(child)) == 0:
            # Leaf node
            if text:
                extras.setdefault(key_base, text)
            # If attributes exist, store them too
            for ak, av in child.attrib.items():
                if av is None:
                    continue
                extras.setdefault(f"{key_base}@{ak}", str(av).strip())
        else:
            # Has nested nodes; capture a compact representation
            # e.g. <author> has <name>, <uri>
            for sub in list(child):
                sub_local = _strip_ns(sub.tag)
                sub_text = (sub.text or "").strip()
                sub_prefix = None
                if sub.tag.startswith("{"):
                    ns_uri2 = sub.tag[1:].split("}", 1)[0]
                    for k, v in NS.items():
                        if v == ns_uri2:
                            sub_prefix = k
                            break
                sub_key = f"{sub_prefix}:{sub_local}" if sub_prefix else sub_local
                if sub_text:
                    extras.setdefault(f"{key_base}.{sub_key}", sub_text)
                for ak, av in sub.attrib.items():
                    if av is None:
                        continue
                    extras.setdefault(f"{key_base}.{sub_key}@{ak}", str(av).strip())

    return extras


def parse_reviews(xml_text: str, app_id: str) -> list:
    """
    Parse Atom XML and extract review rows.
    Returns list of dicts with consistent keys (missing -> empty string).
    """
    rows = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError as e:
        raise ValueError(f"Failed to parse XML: {e}")

    # Atom feed entries
    entries = root.findall("atom:entry", NS)
    for entry in entries:
        # Identify review-like entries:
        # - rating: im:rating
        # - body: atom:content
        rating = _find_text(entry, "im:rating", "")
        body = _find_text(entry, "atom:content", "")

        # The first entry is often the app itself, not a review. Skip if no rating or body.
        if not rating and not body:
            continue

        # Core fields
        review_id = _find_text(entry, "atom:id", "")
        title = _find_text(entry, "atom:title", "")
        updated_at = _find_text(entry, "atom:updated", "")
        author_name = _find_text(entry, "atom:author/atom:name", "")
        author_uri = _find_text(entry, "atom:author/atom:uri", "")

        version = _find_text(entry, "im:version", "")
        vote_sum = _find_text(entry, "im:voteSum", "")
        vote_count = _find_text(entry, "im:voteCount", "")

        # Some feeds have "link rel=related/alternate" etc. Capture useful ones.
        link_alt = ""
        for link in entry.findall("atom:link", NS):
            rel = (link.attrib.get("rel") or "").strip()
            href = (link.attrib.get("href") or "").strip()
            if rel == "alternate" and href:
                link_alt = href
                break

        row = {
            "app_id": app_id,
            "storefront": STOREFRONT,
            "review_id": review_id,
            "author_name": author_name,
            "author_uri": author_uri,
            "title": title,
            "body": body,
            "rating": rating,
            "version": version,
            "updated_at": updated_at,
            "vote_sum": vote_sum,
            "vote_count": vote_count,
            "link_alternate": link_alt,
            # Device-type: generally not present in public feed; keep placeholder if later discovered
            "device_type": "",
        }

        # Gather extra fields (best-effort)
        extras = _collect_extra_fields(entry)
        # Add extras that are not already included as first-class fields
        for k, v in extras.items():
            if k not in row:
                row[k] = v

        rows.append(row)

    return rows


def collect_reviews(app_id: str, max_reviews: int = 100) -> list:
    """
    Paginate feed pages until we have max_reviews or no more reviews.
    """
    collected = []
    page = 1

    while len(collected) < max_reviews:
        try:
            xml_text = fetch_feed(app_id, page)
        except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:
            print(f"[ERROR] Failed to fetch page {page}: {e}")
            return collected
        except Exception as e:
            print(f"[ERROR] Unexpected error fetching page {page}: {e}")
            return collected

        print(f"page {page} fetched")

        try:
            rows = parse_reviews(xml_text, app_id=app_id)
        except Exception as e:
            print(f"[ERROR] Failed to parse page {page}: {e}")
            return collected

        if not rows:
            print("No more reviews found in feed; stopping.")
            break

        # Append, but don’t exceed max_reviews
        for r in rows:
            if len(collected) >= max_reviews:
                break
            collected.append(r)

        print(f"{len(collected)} reviews collected")

        # If we got fewer rows than typical, next page might be empty — still try, but avoid infinite loops.
        page += 1
        if page > 50:  # hard guard
            print("Reached page limit guard (50). Stopping.")
            break

    return collected


def save_csv(rows: list, filename: str) -> str:
    """
    Save rows (list of dicts) to CSV with union of all keys as header.
    Missing fields become empty strings.
    """
    # Determine all columns across rows
    all_keys = []
    seen = set()
    # Put core keys first, then extras in sorted order
    core_order = [
        "app_id", "storefront", "review_id",
        "author_name", "author_uri",
        "title", "body", "rating", "version",
        "updated_at",
        "vote_sum", "vote_count",
        "helpful", "device_type", "link_alternate",
    ]

    for k in core_order:
        if k not in seen:
            all_keys.append(k); seen.add(k)

    # Add remaining keys discovered
    extra_keys = set()
    for r in rows:
        for k in r.keys():
            if k not in seen:
                extra_keys.add(k)
    for k in sorted(extra_keys):
        all_keys.append(k)

    with open(filename, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=all_keys, extrasaction="ignore")
        w.writeheader()
        for r in rows:
            # Ensure all keys exist
            out = {k: r.get(k, "") for k in all_keys}
            w.writerow(out)

    return filename


def _print_table(rows: list, n: int = 5, max_col_width: int = 40):
    """
    Print first n rows as a simple aligned table (no pandas).
    Shows a subset of columns for readability, but includes core info.
    """
    if not rows:
        print("(no rows)")
        return

    cols = ["review_id", "author_name", "rating", "version", "updated_at", "title"]
    sample = rows[:n]

    # Prepare cell strings
    def clip(s):
        s = "" if s is None else str(s)
        s = s.replace("\n", " ").replace("\r", " ")
        if len(s) > max_col_width:
            return s[: max_col_width - 1] + "…"
        return s

    table = []
    for r in sample:
        table.append([clip(r.get(c, "")) for c in cols])

    # Compute widths
    widths = [len(c) for c in cols]
    for row in table:
        for i, cell in enumerate(row):
            widths[i] = max(widths[i], len(cell))

    # Print header
    header = " | ".join(cols[i].ljust(widths[i]) for i in range(len(cols)))
    sep = "-+-".join("-" * widths[i] for i in range(len(cols)))
    print(header)
    print(sep)
    for row in table:
        print(" | ".join(row[i].ljust(widths[i]) for i in range(len(cols))))


# ---------------------------
# Run
# ---------------------------
try:
    app_id = extract_app_id(APP_URL)
except Exception as e:
    print(f"[ERROR] {e}")
    sys.exit(1)

print(f"App ID: {app_id}")
print(f"Storefront: {STOREFRONT}")
print(f"Max reviews: {MAX_REVIEWS}")

reviews = collect_reviews(app_id, MAX_REVIEWS)

if not reviews:
    print("No reviews collected. Exiting.")
    sys.exit(0)

ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_UTC")
filename = f"/content/appstore_reviews_{app_id}_{ts}.csv"

save_csv(reviews, filename)

print("\nFirst 5 rows (preview):")
_print_table(reviews, n=5)

print(f"\nTotal rows saved: {len(reviews)}")
print(f"Saved CSV: {filename}")

# Trigger download in Colab
try:
    from google.colab import files
    files.download(filename)
except Exception as e:
    print(f"[WARN] Could not auto-download (are you in Colab?): {e}")
    print("You can manually download the file from the left sidebar (Files) in Colab.")